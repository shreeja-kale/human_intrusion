import cv2
import os
import time
import gc
import numpy as np
import threading

# --- Setup paths ---
output_folder = "/home/root/human_images/"
os.makedirs(output_folder, exist_ok=True)

# --- Load Model ---
prototxt_path = "mobilenet_ssd/MobileNetSSD_deploy.prototxt"
model_path = "mobilenet_ssd/MobileNetSSD_deploy.caffemodel"
net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)

CLASSES = ["background", "aeroplane", "bicycle", "bird", "boat",
           "bottle", "bus", "car", "cat", "chair", "cow",
           "diningtable", "dog", "horse", "motorbike", "person",
           "pottedplant", "sheep", "sofa", "train", "tvmonitor"]

# --- Camera ---
RTSP_URL = "rtsp://vmukti:vmukti%23907@192.168.1.88/1"
cam = cv2.VideoCapture(RTSP_URL, cv2.CAP_FFMPEG)
cam.set(cv2.CAP_PROP_BUFFERSIZE, 1)

img_counter = 0
capture_interval = 30  # seconds
last_capture_time = time.time()

button_top_left = (100, 120)
button_bottom_right = (300, 170)
clicked_inside = False


def sharpness_score(img):
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    lap = cv2.Laplacian(gray, cv2.CV_64F)
    return lap.var()


def color_variance(img):
    stddev = cv2.meanStdDev(img)[1]
    return np.mean(stddev)


def show_camera_covered_popup():
    popup = 255 * np.ones((200, 400, 3), dtype=np.uint8)
    cv2.putText(popup, "Camera May Be Covered!", (30, 100),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    cv2.imshow("Camera Covered Alert", popup)
    cv2.waitKey(5000)
    cv2.destroyWindow("Camera Covered Alert")


def show_human_alert_popup(person_count, image_path):
    popup = 255 * np.ones((200, 400, 3), dtype=np.uint8)
    cv2.putText(popup, f"Detected: {person_count} person(s)", (40, 70),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    cv2.imshow("Human Detected Alert", popup)
    cv2.waitKey(5000)
    cv2.destroyWindow("Human Detected Alert")

    img = cv2.imread(image_path)
    cv2.imshow("Captured Human", img)
    cv2.waitKey(5000)
    cv2.destroyWindow("Captured Human")


# --- Main Loop ---
while True:
    ret, frame = cam.read()
    if not ret or frame is None:
        print("[‚ö†Ô∏è] Frame grab failed. Reconnecting...")
        cam.release()
        time.sleep(2)
        cam = cv2.VideoCapture(RTSP_URL, cv2.CAP_FFMPEG)
        cam.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        continue

    frame = cv2.resize(frame, (960, 540))

    # --- Camera Covered Check ---
    sharp = sharpness_score(frame)
    color_std = color_variance(frame)

    if sharp < 90 and color_std < 90:
        print("[üö®] Camera may be covered.")
        threading.Thread(target=show_camera_covered_popup, daemon=True).start()

    current_time = time.time()
    if current_time - last_capture_time >= capture_interval:
        (h, w) = frame.shape[:2]
        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)
        net.setInput(blob)
        detections = net.forward()

        boxes = []
        confidences = []

        for i in range(detections.shape[2]):
            confidence = detections[0, 0, i, 2]
            if confidence > 0.4:
                idx = int(detections[0, 0, i, 1])
                if CLASSES[idx] == "person":
                    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                    (startX, startY, endX, endY) = box.astype("int")
                    if (endX - startX) > 50 and (endY - startY) > 50:
                        boxes.append([startX, startY, endX, endY])
                        confidences.append(float(confidence))

        indices = cv2.dnn.NMSBoxes(
            bboxes=[[x, y, ex - x, ey - y] for x, y, ex, ey in boxes],
            scores=confidences,
            score_threshold=0.4,
            nms_threshold=0.3
        )

        if len(indices) > 0:
            person_count = len(indices)
            for i in indices:
                i = i[0] if isinstance(i, (list, tuple, np.ndarray)) else i
                (startX, startY, endX, endY) = boxes[i]
                cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)
                label = f"Person: {confidences[i]*100:.1f}%"
                cv2.putText(frame, label, (startX, startY - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            img_name = f"human_capture_{img_counter}.jpg"
            save_path = os.path.join(output_folder, img_name)
            cv2.imwrite(save_path, frame)
            print(f"[üö®] Human detected. Count: {person_count}. Saved: {save_path}")

            threading.Thread(target=show_human_alert_popup,
                             args=(person_count, save_path), daemon=True).start()
            img_counter += 1
        else:
            print("[‚ÑπÔ∏è] No human detected.")

        last_capture_time = current_time
        del blob, detections
        gc.collect()

    if cv2.waitKey(1) & 0xFF == ord('q'):
        print("üëã Exiting.")
        break

cam.release()
cv2.destroyAllWindows()
